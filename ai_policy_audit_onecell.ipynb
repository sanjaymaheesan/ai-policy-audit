{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "flDe16cZmflP",
        "outputId": "8daaca4b-bcbf-48b1-9479-1ac76749324c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "google_eu.html -> google_eu.txt | paragraphs=469\n",
            "google_us.html -> google_us.txt | paragraphs=260\n",
            "meta_eu.html -> meta_eu.txt | paragraphs=1458\n",
            "meta_us.html -> meta_us.txt | paragraphs=1020\n",
            "openai_eu.html -> openai_eu.txt | paragraphs=86\n",
            "openai_us.html -> openai_us.txt | paragraphs=88\n",
            "xai_eu.html -> xai_eu.txt | paragraphs=50\n",
            "xai_us.html -> xai_us.txt | paragraphs=113\n",
            "\n",
            "Policy metadata:\n",
            " company region          file  n_paras  n_words\n",
            " google     eu google_eu.txt      469    15823\n",
            " google     us google_us.txt      260     8309\n",
            "   meta     eu   meta_eu.txt     1458    27815\n",
            "   meta     us   meta_us.txt     1020    19013\n",
            " openai     eu openai_eu.txt       86     2891\n",
            " openai     us openai_us.txt       88     3163\n",
            "    xai     eu    xai_eu.txt       50     1938\n",
            "    xai     us    xai_us.txt      113     5704\n",
            "Vocab sizes — baseline: 21170 | ablated: 20530\n",
            "\n",
            "==== FINAL SUMMARY ====\n",
            "company region  n_paras  n_words\n",
            " google     eu      469    15823\n",
            " google     us      260     8309\n",
            "   meta     eu     1458    27815\n",
            "   meta     us     1020    19013\n",
            " openai     eu       86     2891\n",
            " openai     us       88     3163\n",
            "    xai     eu       50     1938\n",
            "    xai     us      113     5704\n",
            "\n",
            "Primary topic stability (K=6, seed=42): mean Jaccard = 0.838\n",
            "Classifier LOGO: accuracy = 0.700 ± 0.334 | macro-F1 = 0.214 ± 0.096\n",
            "\n",
            "Rights per 1k (document mean by company×region):\n",
            "company region  rights_per_1000_RAW\n",
            " google     eu             0.252797\n",
            " google     us             0.240703\n",
            "   meta     eu             1.186410\n",
            "   meta     us             0.631147\n",
            " openai     eu             1.729505\n",
            " openai     us             1.896933\n",
            "    xai     eu             2.063983\n",
            "    xai     us             1.051893\n",
            "\n",
            "US state-appendix deltas (per 1k):\n",
            "company  rights_raw  rights_after  flagged\n",
            " google    0.240703      0.241284        1\n",
            "   meta    0.631147      0.631147        0\n",
            " openai    1.896933      1.928640        1\n",
            "    xai    1.051893      1.062323        1\n",
            "\n",
            "Modality aggregates (mean per region):\n",
            "region  org_discretion_per_1k  user_agency_per_1k  user_constraints_per_1k\n",
            "    eu               2.991818            5.065557                      0.0\n",
            "    us               7.562106            4.927657                      0.0\n",
            "\n",
            "Saved figures → outputs/figures; tables → outputs/tables; meta → outputs/meta\n",
            "Supplement bundle: outputs/supplement/supplement_bundle_tp_robustness.zip\n",
            "Supplement SHA256: 80f17581e4b11c88188d68a368d1ffe569f2a1d3162f7d1f372656a64b5dc800\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-832259732.py:566: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  (OUT_META/\"manifest.json\").write_text(json.dumps({\"generated_at\": datetime.datetime.utcnow().isoformat()+\"Z\",\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7243ebdd-63e3-4bf2-a980-27079255bc9b\", \"ai_policy_audit_ALL_20251106_095927.zip\", 1628727)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# === AI Privacy Policies Audit — ONE-CELL (2025-11) ===\n",
        "# Inputs: Put at least one of HTML or TXT for each stem in working dir:\n",
        "#   google_eu.(html|txt), google_us.(html|txt), meta_eu.(html|txt), meta_us.(html|txt),\n",
        "#   openai_eu.(html|txt),  openai_us.(html|txt),  xai_eu.(html|txt),  xai_us.(html|txt)\n",
        "# Output tree: ./outputs/{figures,tables,meta,supplement}\n",
        "# Colab-safe: installs bs4/lxml/regex only. matplotlib only; one plot/figure; no custom colors.\n",
        "\n",
        "# ---------- Minimal installs ----------\n",
        "!pip -q install beautifulsoup4 lxml regex\n",
        "\n",
        "# ---------- Imports & setup ----------\n",
        "import os, sys, json, zipfile, hashlib, shutil, warnings, datetime\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "from bs4 import BeautifulSoup, XMLParsedAsHTMLWarning\n",
        "import regex as re\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.stats import mannwhitneyu\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.model_selection import LeaveOneGroupOut, StratifiedKFold\n",
        "\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ---------- Paths ----------\n",
        "OUT_FIG = Path(\"outputs/figures\"); OUT_TAB = Path(\"outputs/tables\")\n",
        "OUT_META = Path(\"outputs/meta\"); OUT_SUPP = Path(\"outputs/supplement\")\n",
        "for d in (OUT_FIG, OUT_TAB, OUT_META, OUT_SUPP): d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# ---------- Env capture ----------\n",
        "env = {\n",
        "    \"python\": sys.version,\n",
        "    \"numpy\": np.__version__,\n",
        "    \"pandas\": pd.__version__,\n",
        "    \"scikit_learn\": __import__(\"sklearn\").__version__,\n",
        "    \"matplotlib\": matplotlib.__version__,\n",
        "    \"scipy\": __import__(\"scipy\").__version__,\n",
        "    \"bs4\": __import__(\"bs4\").__version__,\n",
        "    \"lxml\": __import__(\"lxml\").__version__,\n",
        "    \"regex\": re.__version__,\n",
        "}\n",
        "(OUT_META/\"env_versions.json\").write_text(json.dumps(env, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "# ---------- IO map ----------\n",
        "PAIR_MAP = {\n",
        "    \"google_eu\": (\"google\",\"eu\"),\n",
        "    \"google_us\": (\"google\",\"us\"),\n",
        "    \"meta_eu\":   (\"meta\",\"eu\"),\n",
        "    \"meta_us\":   (\"meta\",\"us\"),\n",
        "    \"openai_eu\": (\"openai\",\"eu\"),\n",
        "    \"openai_us\": (\"openai\",\"us\"),\n",
        "    \"xai_eu\":    (\"xai\",\"eu\"),\n",
        "    \"xai_us\":    (\"xai\",\"us\"),\n",
        "}\n",
        "EXPECTED_STEMS = list(PAIR_MAP.keys())\n",
        "\n",
        "# ---------- HTML -> TXT (keeping blank-line semantics downstream) ----------\n",
        "def smart_soup(html_text: str) -> BeautifulSoup:\n",
        "    head = html_text.lstrip()[:4096].lower()\n",
        "    is_xml = (\n",
        "        head.startswith(\"<?xml\") or\n",
        "        bool(re.search(r'<(rss|feed|sitemapindex|urlset)\\b', head)) or\n",
        "        bool(re.search(r'<html\\b[^>]*xmlns=', head))\n",
        "    )\n",
        "    if not is_xml:\n",
        "        warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
        "    return BeautifulSoup(html_text, \"xml\" if is_xml else \"lxml\")\n",
        "\n",
        "NOISE_SELECTORS = [\n",
        "    \"header\",\"footer\",\"nav\",\"aside\",\"[role=navigation]\",\"[role=dialog]\",\n",
        "    \".cookie\",\".cookies\",\".consent\",\".banner\",\".gdpr\",\".truste\"\n",
        "]\n",
        "\n",
        "def html_to_txt(in_path: Path, out_path: Path, min_chars=30):\n",
        "    html = in_path.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    soup = smart_soup(html)\n",
        "    for t in soup([\"script\",\"style\",\"noscript\"]): t.decompose()\n",
        "    for sel in NOISE_SELECTORS:\n",
        "        for el in soup.select(sel): el.decompose()\n",
        "    for br in soup.find_all(\"br\"): br.replace_with(\"\\n\")\n",
        "    blocks = []\n",
        "    # Use structural blocks, then rebuild paragraphs with \\n\\n\n",
        "    for tag in soup.find_all([\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"p\",\"li\",\"dt\",\"dd\"]):\n",
        "        text = tag.get_text(\" \", strip=True)\n",
        "        if text:\n",
        "            text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "            if len(text) >= min_chars:\n",
        "                blocks.append(text)\n",
        "    txt = \"\\n\\n\".join(blocks)\n",
        "    out_path.write_text(txt, encoding=\"utf-8\")\n",
        "    print(f\"{in_path.name} -> {out_path.name} | paragraphs={len(blocks)}\")\n",
        "\n",
        "# Convert any available HTML → TXT (without overwriting existing .txt)\n",
        "for stem in EXPECTED_STEMS:\n",
        "    html_p, txt_p = Path(f\"{stem}.html\"), Path(f\"{stem}.txt\")\n",
        "    if html_p.exists() and not txt_p.exists():\n",
        "        html_to_txt(html_p, txt_p)\n",
        "    elif not html_p.exists() and not txt_p.exists():\n",
        "        print(f\"WARNING: Missing both {stem}.html and {stem}.txt\")\n",
        "\n",
        "# Validate we have TXT for all stems\n",
        "missing = [f\"{s}.txt\" for s in EXPECTED_STEMS if not Path(f\"{s}.txt\").exists()]\n",
        "if missing:\n",
        "    raise SystemExit(\"ERROR: Missing required TXT files: \" + \", \".join(missing))\n",
        "\n",
        "# ---------- Strict paragraphing on \\n\\n+, keep >=30 chars ----------\n",
        "def split_paragraphs_strict(raw: str):\n",
        "    # standardize newlines, then split on blank lines\n",
        "    raw = raw.replace(\"\\r\\n\",\"\\n\").replace(\"\\r\",\"\\n\")\n",
        "    parts = re.split(r\"\\n\\n+\", raw)\n",
        "    return [p.strip() for p in parts if len(p.strip()) >= 30]\n",
        "\n",
        "rows = []\n",
        "for stem, (company, region) in PAIR_MAP.items():\n",
        "    raw = Path(f\"{stem}.txt\").read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
        "    paras = split_paragraphs_strict(raw)\n",
        "    for p in paras:\n",
        "        n_words = len(re.findall(r\"\\b\\w+\\b\", p))\n",
        "        rows.append({\"file\": f\"{stem}.txt\", \"company\": company, \"region\": region,\n",
        "                     \"paragraph\": p, \"n_words\": n_words})\n",
        "df = pd.DataFrame(rows)\n",
        "if df.empty: raise SystemExit(\"ERROR: No paragraphs after strict split.\")\n",
        "\n",
        "meta = (df.groupby([\"company\",\"region\",\"file\"])\n",
        "          .agg(n_paras=(\"paragraph\",\"size\"), n_words=(\"n_words\",\"sum\"))\n",
        "          .reset_index())\n",
        "meta.to_csv(OUT_META/\"policy_metadata.csv\", index=False)\n",
        "print(\"\\nPolicy metadata:\\n\", meta.to_string(index=False))\n",
        "\n",
        "# ---------- Cleaning & brand ablation ----------\n",
        "def clean_text(s: str) -> str:\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"[^a-z0-9\\s]+\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "df[\"text_clean\"] = df[\"paragraph\"].apply(clean_text)\n",
        "\n",
        "brand_lexicon = [\n",
        "  \"google\", \"meta\", \"openai\", \"xai\", \"x.ai\",\n",
        "  \"gemini\", \"bard\", \"chatgpt\", \"dall-e\", \"gpt\", \"gpt-4o\", \"gpt 4o\", \"gpt4o\",\n",
        "  \"llama\", \"threads\", \"instagram\", \"facebook\", \"whatsapp\",\n",
        "  \"youtube\", \"you tube\", \"deepmind\", \"vertex ai\", \"palm\", \"med-palm\",\n",
        "  \"google llc\", \"meta platforms\", \"openai lp\", \"xai corp\"\n",
        "]\n",
        "def build_brand_union_regex(terms):\n",
        "    escaped = []\n",
        "    for t in terms:\n",
        "        t = t.lower()\n",
        "        t = re.escape(t).replace(\"\\\\ \", r\"\\\\s+\")\n",
        "        escaped.append(rf\"\\b{t}\\b\")\n",
        "    return re.compile(\"|\".join(escaped), flags=re.IGNORECASE)\n",
        "brand_union_re = build_brand_union_regex(brand_lexicon)\n",
        "\n",
        "df[\"text_ablated\"] = df[\"text_clean\"].apply(lambda s: brand_union_re.sub(\" \", s))\n",
        "df[\"text_ablated\"] = df[\"text_ablated\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
        "\n",
        "# ---------- TF–IDF (paragraph level, baseline & ablated) ----------\n",
        "TFIDF = dict(ngram_range=(1,3), stop_words=\"english\", norm=\"l2\", smooth_idf=True, min_df=2)\n",
        "vec_base = TfidfVectorizer(**TFIDF)\n",
        "X_base = vec_base.fit_transform(df[\"text_clean\"].values)\n",
        "vec_abla = TfidfVectorizer(**TFIDF)\n",
        "X_abla = vec_abla.fit_transform(df[\"text_ablated\"].values)\n",
        "\n",
        "print(\"Vocab sizes — baseline:\", len(vec_base.get_feature_names_out()),\n",
        "      \"| ablated:\", len(vec_abla.get_feature_names_out()))\n",
        "\n",
        "# ---------- Topics & stability (NMF; top-12 Jaccard; Hungarian) ----------\n",
        "def top_terms(model, vocab, topn=12):\n",
        "    tops = []\n",
        "    for row in model.components_:\n",
        "        idx = np.argsort(row)[::-1][:topn]\n",
        "        tops.append([vocab[i] for i in idx])\n",
        "    return tops\n",
        "\n",
        "def jaccard(a, b):\n",
        "    A, B = set(a), set(b)\n",
        "    return len(A & B) / max(1, len(A | B))\n",
        "\n",
        "K_GRID = [6,7,8,9]; SEEDS = [13,29,42]\n",
        "rows_grid = []\n",
        "for K in K_GRID:\n",
        "    for sd in SEEDS:\n",
        "        nmf_b = NMF(n_components=K, init=\"nndsvda\", solver=\"cd\", random_state=sd, max_iter=2000)\n",
        "        Wb = nmf_b.fit_transform(X_base)\n",
        "        Tb = top_terms(nmf_b, vec_base.get_feature_names_out(), topn=12)\n",
        "        nmf_a = NMF(n_components=K, init=\"nndsvda\", solver=\"cd\", random_state=sd, max_iter=2000)\n",
        "        Wa = nmf_a.fit_transform(X_abla)\n",
        "        Ta = top_terms(nmf_a, vec_abla.get_feature_names_out(), topn=12)\n",
        "        cost = np.zeros((K, K))\n",
        "        for i in range(K):\n",
        "            for j in range(K):\n",
        "                cost[i,j] = 1.0 - jaccard(Tb[i], Ta[j])\n",
        "        ri, cj = linear_sum_assignment(cost)\n",
        "        matched = [1.0 - cost[i,j] for i,j in zip(ri,cj)]\n",
        "        rows_grid.append({\"K\":K, \"seed\":sd, \"mean_jaccard\": float(np.mean(matched))})\n",
        "grid_df = pd.DataFrame(rows_grid)\n",
        "grid_df.to_csv(OUT_TAB/\"S1_topic_stability_grid.csv\", index=False)\n",
        "\n",
        "# Primary (K=6, seed=42)\n",
        "Kp, Sp = 6, 42\n",
        "nmf_b_p = NMF(n_components=Kp, init=\"nndsvda\", solver=\"cd\", random_state=Sp, max_iter=2000)\n",
        "Wb_p = nmf_b_p.fit_transform(X_base)\n",
        "Tb_p = top_terms(nmf_b_p, vec_base.get_feature_names_out(), topn=12)\n",
        "nmf_a_p = NMF(n_components=Kp, init=\"nndsvda\", solver=\"cd\", random_state=Sp, max_iter=2000)\n",
        "Wa_p = nmf_a_p.fit_transform(X_abla)\n",
        "Ta_p = top_terms(nmf_a_p, vec_abla.get_feature_names_out(), topn=12)\n",
        "mj_primary = float(grid_df.query(\"K==6 and seed==42\")[\"mean_jaccard\"].iloc[0])\n",
        "\n",
        "def company_topic_loads(W, labels):\n",
        "    lab = labels.values\n",
        "    uniq = sorted(np.unique(lab))\n",
        "    rows = []\n",
        "    for u in uniq:\n",
        "        sel = (lab == u)\n",
        "        mean_loads = W[sel].mean(axis=0)\n",
        "        rows.append([u] + list(np.asarray(mean_loads).ravel()))\n",
        "    cols = [\"company\"] + [f\"topic_{i+1}\" for i in range(W.shape[1])]\n",
        "    return pd.DataFrame(rows, columns=cols)\n",
        "\n",
        "A1 = company_topic_loads(Wb_p, df[\"company\"])\n",
        "A2 = company_topic_loads(Wa_p, df[\"company\"])\n",
        "A3 = pd.DataFrame({\"topic\":[f\"topic_{i+1}\" for i in range(Kp)], \"top_terms\":[\", \".join(t) for t in Tb_p]})\n",
        "A4 = pd.DataFrame({\"topic\":[f\"topic_{i+1}\" for i in range(Kp)], \"top_terms\":[\", \".join(t) for t in Ta_p]})\n",
        "A1.to_csv(OUT_TAB/\"A1_topic_loads_baseline_K6.csv\", index=False)\n",
        "A2.to_csv(OUT_TAB/\"A2_topic_loads_ablated_K6.csv\", index=False)\n",
        "A3.to_csv(OUT_TAB/\"A3_top_terms_baseline_K6.csv\", index=False)\n",
        "A4.to_csv(OUT_TAB/\"A4_top_terms_ablated_K6.csv\", index=False)\n",
        "\n",
        "# Figures 1A/1B\n",
        "def plot_topic_bars(df_loads, title, outbase):\n",
        "    companies = df_loads[\"company\"].tolist()\n",
        "    topics = [c for c in df_loads.columns if c.startswith(\"topic_\")]\n",
        "    K = len(topics)\n",
        "    x = np.arange(len(companies)); width = 0.8 / max(K, 1)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, t in enumerate(topics):\n",
        "        plt.bar(x + i*width - (K-1)*width/2, df_loads[t].values, width=width, label=t)\n",
        "    plt.xticks(x, companies, rotation=0); plt.legend(); plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_FIG/f\"{outbase}.png\", dpi=300)\n",
        "    plt.savefig(OUT_FIG/f\"{outbase}.svg\"); plt.close()\n",
        "\n",
        "plot_topic_bars(A1, \"Topic profiles (K=6) by company — baseline TF–IDF\", \"fig01A_topics_baseline\")\n",
        "plot_topic_bars(A2, \"Topic profiles (K=6) by company — brand ablation\", \"fig01B_topics_ablated\")\n",
        "(OUT_FIG/\"fig01A_topics_baseline.txt\").write_text(\n",
        "    \"Topic profiles (K=6) by company using baseline TF–IDF. Bars show per-company loads across six topics.\", encoding=\"utf-8\")\n",
        "(OUT_FIG/\"fig01B_topics_ablated.txt\").write_text(\n",
        "    \"Topic profiles (K=6) after brand ablation. Persistence indicates ecosystem vocabularies beyond proper names.\", encoding=\"utf-8\")\n",
        "for fp in (\"fig01A_topics_baseline\",\"fig01B_topics_ablated\"):\n",
        "    (OUT_FIG/f\"{fp}.txt\").write_text((OUT_FIG/f\"{fp}.txt\").read_text(encoding=\"utf-8\")+f\"\\nPrimary stability: mean Jaccard (K=6, seed=42) = {mj_primary:.3f}\", encoding=\"utf-8\")\n",
        "\n",
        "# ---------- Classifier (LOGO by document on ablated features) ----------\n",
        "y = df[\"company\"].values\n",
        "groups = df[\"file\"].values  # each held-out doc is one file\n",
        "logo = LeaveOneGroupOut()\n",
        "fold_rows = []; y_true_all=[]; y_pred_all=[]\n",
        "for fold, (tr, te) in enumerate(logo.split(X_abla, y, groups=groups), start=1):\n",
        "    clf = LogisticRegression(solver=\"lbfgs\", class_weight=\"balanced\", max_iter=5000, random_state=SEED)\n",
        "    clf.fit(X_abla[tr], y[tr])\n",
        "    yhat = clf.predict(X_abla[te])\n",
        "    acc = accuracy_score(y[te], yhat); f1m = f1_score(y[te], yhat, average=\"macro\")\n",
        "    fold_rows.append({\"fold\": fold, \"heldout_file\": df.iloc[te[0]][\"file\"], \"accuracy\": acc, \"macro_f1\": f1m})\n",
        "    y_true_all.extend(y[te]); y_pred_all.extend(yhat)\n",
        "df_folds = pd.DataFrame(fold_rows)\n",
        "acc_mean, acc_sd = df_folds[\"accuracy\"].mean(), df_folds[\"accuracy\"].std(ddof=1)\n",
        "f1_mean,  f1_sd  = df_folds[\"macro_f1\"].mean(), df_folds[\"macro_f1\"].std(ddof=1)\n",
        "\n",
        "# Tidy classification report (robust to float 'accuracy')\n",
        "rep = classification_report(y_true_all, y_pred_all, output_dict=True, zero_division=0)\n",
        "rep_rows = []\n",
        "for key, val in rep.items():\n",
        "    row = {\"class\": key}\n",
        "    if isinstance(val, dict):\n",
        "        for k in (\"precision\",\"recall\",\"f1-score\",\"support\"):\n",
        "            if k in val: row[k] = val[k]\n",
        "    elif isinstance(val, (float,int)):\n",
        "        row[\"score\"] = float(val)\n",
        "    rep_rows.append(row)\n",
        "pd.DataFrame(rep_rows).to_csv(OUT_TAB/\"S2_classifier_report_LOGO.csv\", index=False)\n",
        "\n",
        "labels = sorted(np.unique(y))\n",
        "cm = confusion_matrix(y_true_all, y_pred_all, labels=labels)\n",
        "cm_df = pd.DataFrame(cm, index=[f\"true_{l}\" for l in labels], columns=[f\"pred_{l}\" for l in labels])\n",
        "cm_df.to_csv(OUT_TAB/\"S3_classifier_confusion_LOGO.csv\")\n",
        "\n",
        "# Optimistic sensitivity (5-fold, no grouping)\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "accs_s, f1s_s = [], []\n",
        "for tr, te in skf.split(X_abla, y):\n",
        "    clf = LogisticRegression(solver=\"lbfgs\", class_weight=\"balanced\", max_iter=5000, random_state=SEED)\n",
        "    clf.fit(X_abla[tr], y[tr])\n",
        "    yp = clf.predict(X_abla[te])\n",
        "    accs_s.append(accuracy_score(y[te], yp))\n",
        "    f1s_s.append(f1_score(y[te], yp, average=\"macro\"))\n",
        "sdf = pd.DataFrame({\"accuracy\":accs_s, \"macro_f1\":f1s_s})\n",
        "sdf.to_csv(OUT_TAB/\"S3b_classifier_summary_sensitivity.csv\", index=False)\n",
        "\n",
        "# Table 1 (stability + classifiers)\n",
        "tab1 = pd.DataFrame([\n",
        "    {\"row\":\"Topic stability (Jaccard), K=6, seed=42\", \"mean_jaccard\": mj_primary},\n",
        "    {\"row\":\"classifier_primary_LOGO\", \"accuracy_mean\": acc_mean, \"accuracy_sd\": acc_sd,\n",
        "     \"macro_f1_mean\": f1_mean, \"macro_f1_sd\": f1_sd},\n",
        "    {\"row\":\"classifier_sensitivity_5fold\", \"accuracy_mean\": sdf[\"accuracy\"].mean(), \"accuracy_sd\": sdf[\"accuracy\"].std(ddof=1),\n",
        "     \"macro_f1_mean\": sdf[\"macro_f1\"].mean(), \"macro_f1_sd\": sdf[\"macro_f1\"].std(ddof=1)},\n",
        "])\n",
        "tab1.to_csv(OUT_TAB/\"Table_1_model_stability_and_classifier.csv\", index=False)\n",
        "\n",
        "# ---------- Rights & consent per 1,000 with bootstrap CIs (document level) ----------\n",
        "RIGHTS_REGEX = r\"\"\"(?ix)\n",
        "  \\bright(s)?\\s+of\\s+(access|rectification|erasure)\\b |\n",
        "  \\bright(s)?\\s+to\\s+(rectification|erasure|deletion|portability|restriction|object|objection|appeal)\\b |\n",
        "  \\b(deletion|erase|erasure|portability|restriction|objection|appeal)\\b |\n",
        "  \\b(supervisory\\s+authority|data\\s+protection\\s+authority|dpa)\\b\n",
        "\"\"\"\n",
        "CONSENT_REGEX = r\"\"\"(?ix)\n",
        "  \\bconsent(s|ed|ing)?\\b |\n",
        "  \\bwithdraw\\s+consent\\b |\n",
        "  \\bopt[-\\s]?(in|out)\\b |\n",
        "  \\bagree(s|d|ment|ments)?\\b |\n",
        "  \\bchoice(s)?\\b |\n",
        "  \\bpreference(s)?\\b |\n",
        "  \\bsetting(s)?\\b\n",
        "\"\"\"\n",
        "rights_re  = re.compile(RIGHTS_REGEX,  flags=re.IGNORECASE | re.VERBOSE)\n",
        "consent_re = re.compile(CONSENT_REGEX, flags=re.IGNORECASE | re.VERBOSE)\n",
        "\n",
        "df[\"rights_hits\"]  = df[\"paragraph\"].apply(lambda s: len(list(rights_re.finditer(s))))\n",
        "df[\"consent_hits\"] = df[\"paragraph\"].apply(lambda s: len(list(consent_re.finditer(s))))\n",
        "\n",
        "agg = df.groupby([\"company\",\"region\",\"file\"]).agg(\n",
        "    words=(\"n_words\",\"sum\"),\n",
        "    rights=(\"rights_hits\",\"sum\"),\n",
        "    consent=(\"consent_hits\",\"sum\")\n",
        ").reset_index()\n",
        "agg[\"rights_per_1000_RAW\"] = agg[\"rights\"] / agg[\"words\"] * 1000.0\n",
        "den = (agg[\"rights\"] + agg[\"consent\"]).replace(0, np.nan)\n",
        "agg[\"rights_share\"] = (agg[\"rights\"] / den).fillna(0.0)\n",
        "\n",
        "def bootstrap_doc(sub_df, B=2000, seed=SEED):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(sub_df)\n",
        "    rates, shares = [], []\n",
        "    for _ in range(B):\n",
        "        rs = int(rng.integers(0, 1 << 31))\n",
        "        samp = sub_df.sample(n, replace=True, random_state=rs)\n",
        "        w = samp[\"n_words\"].sum(); r = samp[\"rights_hits\"].sum(); c = samp[\"consent_hits\"].sum()\n",
        "        rates.append((r / w * 1000.0) if w>0 else 0.0)\n",
        "        shares.append((r / (r+c)) if (r+c)>0 else 0.0)\n",
        "    lo_rate, hi_rate = np.percentile(rates, [2.5, 97.5])\n",
        "    lo_share, hi_share = np.percentile(shares, [2.5, 97.5])\n",
        "    return lo_rate, hi_rate, lo_share, hi_share\n",
        "\n",
        "cis = []\n",
        "for (comp, reg, file), sub in df.groupby([\"company\",\"region\",\"file\"]):\n",
        "    lo_r, hi_r, lo_s, hi_s = bootstrap_doc(sub)\n",
        "    cis.append({\"company\":comp,\"region\":reg,\"file\":file,\n",
        "                \"rights_per_1000_RAW_lo\": lo_r, \"rights_per_1000_RAW_hi\": hi_r,\n",
        "                \"rights_share_lo\": lo_s, \"rights_share_hi\": hi_s})\n",
        "cis = pd.DataFrame(cis)\n",
        "tab2 = agg.merge(cis, on=[\"company\",\"region\",\"file\"], how=\"left\")\n",
        "tab2.to_csv(OUT_TAB/\"Table_2_rights_rates_RAW.csv\", index=False)\n",
        "tab2.to_csv(OUT_TAB/\"S4_rights_share_per_doc.csv\", index=False)\n",
        "\n",
        "# Figure 2 (document rows with CI bars)\n",
        "labels_lr = [f\"{r.company}-{r.region}\" for _, r in tab2.iterrows()]\n",
        "y = tab2[\"rights_per_1000_RAW\"].values\n",
        "yerr = np.vstack([\n",
        "    y - tab2[\"rights_per_1000_RAW_lo\"].values,\n",
        "    tab2[\"rights_per_1000_RAW_hi\"].values - y\n",
        "])\n",
        "x = np.arange(len(labels_lr))\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(x, y)\n",
        "plt.errorbar(x, y, yerr=yerr, fmt=\"none\", capsize=3)\n",
        "plt.xticks(x, labels_lr, rotation=45, ha=\"right\")\n",
        "plt.ylabel(\"Rights per 1,000 words (RAW)\")\n",
        "plt.title(\"Rights references per 1,000 words (with 95% bootstrap CIs)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_FIG/\"fig02_rights_per_1k_RAW.png\", dpi=300)\n",
        "plt.savefig(OUT_FIG/\"fig02_rights_per_1k_RAW.svg\"); plt.close()\n",
        "(OUT_FIG/\"fig02_rights_per_1k_RAW.txt\").write_text(\n",
        "    \"Rights references per 1,000 words (RAW) with 95% bootstrap intervals; document-level. Company×region aggregates available in S4.\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ---------- Neutral US state-appendix identification (overrideable) ----------\n",
        "DEFAULT_STATE_APP_REGEX = r\"\"\"(?ix)\n",
        "  \\b(ccpa|cpra|california\\s+privacy\\s+rights?\\s+act|california\\s+consumer\\s+privacy\\s+act)\\b |\n",
        "  \\b(vcdpa|virginia\\s+consumer\\s+data\\s+protection\\s+act)\\b |\n",
        "  \\b(cdpa|colorado\\s+privacy\\s+act)\\b |\n",
        "  \\b(ctdpa|connecticut\\s+data\\s+privacy\\s+act)\\b |\n",
        "  \\b(ucpa|utah\\s+consumer\\s+privacy\\s+act)\\b |\n",
        "  \\b(iowa\\s+consumer\\s+data\\s+protection\\s+act)\\b |\n",
        "  \\b(oregon\\s+consumer\\s+privacy\\s+act)\\b |\n",
        "  \\b(texas\\s+data\\s+privacy\\s+and\\s+security\\s+act|tdpsa)\\b |\n",
        "  \\b(montana|delaware|tennessee|indiana|nebraska|new\\s+jersey|new\\s+hampshire|florida)\\s+(consumer\\s+)?privacy\\s+act\\b |\n",
        "  \\b(state[-\\s]?specific\\s+(privacy\\s+)?(notice|notices|disclosure|disclosures|rights))\\b |\n",
        "  \\b(supplemental|additional)\\s+(information|notice|disclosures?)\\s+for\\s+(u\\.?\\s*s\\.?\\s*residents|[a-z]+\\s+residents)\\b |\n",
        "  \\b(notice\\s+to\\s+(california|virginia|colorado|connecticut|utah|iowa|oregon|texas|montana|delaware|tennessee|indiana|nebraska|new\\s+jersey|new\\s+hampshire|florida)\\s+residents)\\b |\n",
        "  \\b(your\\s+california\\s+privacy\\s+rights)\\b |\n",
        "  \\b(do\\s+not\\s+sell|do\\s+not\\s+share|targeted\\s+advertising|sale\\s+of\\s+personal\\s+information)\\b\n",
        "\"\"\"\n",
        "# Allow an override file next to the notebook; also save effective pattern to outputs\n",
        "override_path = Path(\"S5_us_fairness_regex.txt\")\n",
        "pattern_text = override_path.read_text(encoding=\"utf-8\") if override_path.exists() else DEFAULT_STATE_APP_REGEX\n",
        "(OUT_TAB/\"S5_us_fairness_regex.txt\").write_text(pattern_text, encoding=\"utf-8\")\n",
        "state_app_re = re.compile(pattern_text, flags=re.IGNORECASE | re.VERBOSE | re.DOTALL)\n",
        "\n",
        "df[\"us_doc\"] = df[\"region\"].eq(\"us\")\n",
        "df[\"state_appendix_flag\"] = df.apply(lambda r: (r[\"us_doc\"] and bool(state_app_re.search(r[\"paragraph\"]))), axis=1)\n",
        "\n",
        "def rights_per1k(sub):\n",
        "    w = sub[\"n_words\"].sum(); rr = sub[\"rights_hits\"].sum()\n",
        "    return (rr / w * 1000.0) if w>0 else 0.0\n",
        "\n",
        "effects = []\n",
        "for (comp, file), sub in df[df[\"us_doc\"]].groupby([\"company\",\"file\"]):\n",
        "    raw_rate  = rights_per1k(sub)\n",
        "    kept_rate = rights_per1k(sub[~sub[\"state_appendix_flag\"]])\n",
        "    flagged   = int(sub[\"state_appendix_flag\"].any())\n",
        "    effects.append({\"company\":comp,\"file\":file,\"rights_raw\":raw_rate,\"rights_after\":kept_rate,\"flagged\":flagged})\n",
        "effects = pd.DataFrame(effects)\n",
        "effects.to_csv(OUT_TAB/\"S5_state_appendix_effects.csv\", index=False)\n",
        "\n",
        "# Diagnostics (flagged vs kept)\n",
        "diag_rows = []\n",
        "for (comp, file), sub in df[df[\"us_doc\"]].groupby([\"company\",\"file\"]):\n",
        "    all_w = sub[\"n_words\"].sum(); all_r = sub[\"rights_hits\"].sum()\n",
        "    flg = sub[sub[\"state_appendix_flag\"]]; kept = sub[~sub[\"state_appendix_flag\"]]\n",
        "    fw, fr = flg[\"n_words\"].sum(), flg[\"rights_hits\"].sum()\n",
        "    kw, kr = kept[\"n_words\"].sum(), kept[\"rights_hits\"].sum()\n",
        "    def per1k(r,w): return (r/w*1000.0) if w>0 else 0.0\n",
        "    diag_rows.append({\n",
        "      \"company\":comp,\"file\":file,\n",
        "      \"paras_total\": len(sub), \"paras_flagged\": len(flg),\n",
        "      \"rights_per_1k_RAW\": per1k(all_r, all_w),\n",
        "      \"rights_per_1k_FLAGGED_ONLY\": per1k(fr, fw),\n",
        "      \"rights_per_1k_KEPT_ONLY\": per1k(kr, kw),\n",
        "      \"delta_after_minus_raw\": per1k(kr, kw) - per1k(all_r, all_w)\n",
        "    })\n",
        "pd.DataFrame(diag_rows).to_csv(OUT_TAB/\"S5c_state_appendix_diagnostics.csv\", index=False)\n",
        "\n",
        "# Figure 4 (US only, RAW vs adjusted)\n",
        "if not effects.empty:\n",
        "    x = np.arange(effects.shape[0])\n",
        "    plt.figure(figsize=(8,6))\n",
        "    plt.bar(x-0.15, effects[\"rights_raw\"].values, width=0.3, label=\"RAW\")\n",
        "    plt.bar(x+0.15, effects[\"rights_after\"].values, width=0.3, label=\"Adjusted\")\n",
        "    plt.xticks(x, effects[\"company\"], rotation=0); plt.legend()\n",
        "    plt.ylabel(\"Rights per 1,000 words\")\n",
        "    plt.title(\"US state-appendix diagnostic: RAW vs adjusted\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(OUT_FIG/\"fig04_state_appendix_adjustment.png\", dpi=300)\n",
        "    plt.savefig(OUT_FIG/\"fig04_state_appendix_adjustment.svg\"); plt.close()\n",
        "    (OUT_FIG/\"fig04_state_appendix_adjustment.txt\").write_text(\n",
        "        \"US documents: rights per 1,000 words before/after excluding state-law appendix sections identified by regex. EU values unaffected.\",\n",
        "        encoding=\"utf-8\"\n",
        "    )\n",
        "\n",
        "# ---------- Modality markers (+ pronouns, doc level) ----------\n",
        "ORG_DISCRETION = [\n",
        "    \"we may\",\"we reserve the right\",\"we will\",\"at our discretion\",\"in our sole discretion\",\n",
        "    \"we are not responsible\",\"we disclaim\"\n",
        "]\n",
        "USER_AGENCY  = [\"you can\",\"you may\",\"you have the right\",\"you are able to\"]\n",
        "USER_CONSTR  = [\"you may not\",\"you cannot\",\"you must not\",\"prohibited\"]\n",
        "\n",
        "def count_phrase(text, phrase):\n",
        "    return len(list(re.finditer(rf\"\\b{re.escape(phrase)}\\b\", text, flags=re.IGNORECASE)))\n",
        "def per_1000(c, w): return (c/w*1000.0) if w>0 else 0.0\n",
        "\n",
        "mod_rows = []\n",
        "for (comp, reg, file), sub in df.groupby([\"company\",\"region\",\"file\"]):\n",
        "    text = \" \".join(sub[\"paragraph\"].tolist())\n",
        "    words = len(re.findall(r\"\\b\\w+\\b\", text))\n",
        "    org  = sum(count_phrase(text, p) for p in ORG_DISCRETION)\n",
        "    ag   = sum(count_phrase(text, p) for p in USER_AGENCY)\n",
        "    cons = sum(count_phrase(text, p) for p in USER_CONSTR)\n",
        "    mod_rows.append({\n",
        "        \"company\":comp,\"region\":reg,\"file\":file,\n",
        "        \"org_discretion_per_1k\": per_1000(org, words),\n",
        "        \"user_agency_per_1k\": per_1000(ag, words),\n",
        "        \"user_constraints_per_1k\": per_1000(cons, words),\n",
        "        \"we_our\": len(re.findall(r\"\\b(we|our)\\b\", text, flags=re.IGNORECASE)),\n",
        "        \"you_your\": len(re.findall(r\"\\b(you|your)\\b\", text, flags=re.IGNORECASE)),\n",
        "        \"words\": words\n",
        "    })\n",
        "mod = pd.DataFrame(mod_rows)\n",
        "mod.to_csv(OUT_TAB/\"S6_modality_per_doc.csv\", index=False)\n",
        "pd.DataFrame(mod[[\"company\",\"region\",\"file\",\"we_our\",\"you_your\",\"words\"]]).to_csv(OUT_TAB/\"S7_pronouns_per_doc.csv\", index=False)\n",
        "\n",
        "reg = (mod.groupby(\"region\")\n",
        "         .agg(org_discretion_per_1k=(\"org_discretion_per_1k\",\"mean\"),\n",
        "              user_agency_per_1k=(\"user_agency_per_1k\",\"mean\"),\n",
        "              user_constraints_per_1k=(\"user_constraints_per_1k\",\"mean\"))\n",
        "         .reset_index())\n",
        "plt.figure(figsize=(8,6))\n",
        "x = np.arange(reg.shape[0]); w = 0.25\n",
        "plt.bar(x-w, reg[\"org_discretion_per_1k\"], width=w, label=\"Organisational discretion\")\n",
        "plt.bar(x,   reg[\"user_agency_per_1k\"],  width=w, label=\"User agency\")\n",
        "plt.bar(x+w, reg[\"user_constraints_per_1k\"], width=w, label=\"User constraints\")\n",
        "plt.xticks(x, reg[\"region\"]); plt.legend(); plt.title(\"Modality markers per 1,000 words (EU vs US)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_FIG/\"fig03_modality_markers.png\", dpi=300)\n",
        "plt.savefig(OUT_FIG/\"fig03_modality_markers.svg\"); plt.close()\n",
        "(OUT_FIG/\"fig03_modality_markers.txt\").write_text(\n",
        "    \"Modality markers per 1,000 words aggregated by region. Organisational discretion vs user agency vs user constraints.\",\n",
        "    encoding=\"utf-8\"\n",
        ")\n",
        "\n",
        "# ---------- Simple significance tests (doc-level EU vs US) ----------\n",
        "def mwu_row(name, eu, us):\n",
        "    if len(eu)==0 or len(us)==0: return {\"metric\":name, \"stat\": float(\"nan\"), \"p_value\": float(\"nan\")}\n",
        "    stat, p = mannwhitneyu(eu, us, alternative=\"two-sided\")\n",
        "    return {\"metric\":name, \"stat\": float(stat), \"p_value\": float(p)}\n",
        "rows_tests = []\n",
        "rows_tests.append(mwu_row(\"rights_share\",\n",
        "                          tab2.loc[tab2[\"region\"]==\"eu\",\"rights_share\"],\n",
        "                          tab2.loc[tab2[\"region\"]==\"us\",\"rights_share\"]))\n",
        "for col in [\"org_discretion_per_1k\",\"user_agency_per_1k\",\"user_constraints_per_1k\"]:\n",
        "    rows_tests.append(mwu_row(col,\n",
        "                              mod.loc[mod[\"region\"]==\"eu\", col],\n",
        "                              mod.loc[mod[\"region\"]==\"us\", col]))\n",
        "pd.DataFrame(rows_tests).to_csv(OUT_TAB/\"S1b_document_level_tests.csv\", index=False)\n",
        "\n",
        "# ---------- Class balance (Table 0 + fig00) ----------\n",
        "bal = meta[[\"company\",\"region\",\"file\",\"n_paras\"]].copy()\n",
        "bal.to_csv(OUT_TAB/\"Table_0_class_balance.csv\", index=False)\n",
        "plt.figure(figsize=(8,6))\n",
        "labels_cb = bal[\"company\"] + \"-\" + bal[\"region\"]\n",
        "x = np.arange(len(bal)); plt.bar(x, bal[\"n_paras\"].values)\n",
        "plt.xticks(x, labels_cb, rotation=45, ha=\"right\"); plt.title(\"Paragraph count per company–region\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(OUT_FIG/\"fig00_class_balance.png\", dpi=300)\n",
        "plt.savefig(OUT_FIG/\"fig00_class_balance.svg\"); plt.close()\n",
        "(OUT_FIG/\"fig00_class_balance.txt\").write_text(\n",
        "    \"Class balance: paragraph counts per company–region used in the classifier.\", encoding=\"utf-8\")\n",
        "\n",
        "# ---------- Supplement README + bundle + manifest ----------\n",
        "readme = (\n",
        "  \"Supplement bundle — AI Privacy Policies Audit (Nov 2025)\\n\"\n",
        "  \"Methods: strict \\\\n\\\\n+ paragraphing (>=30 chars); TF–IDF(1–3,min_df=2,L2,English), brand ablation via union regex;\\n\"\n",
        "  \"NMF topics (K=6 primary, seed=42); Hungarian match on top-12 Jaccard; LOGO classifier on ablated features; 5-fold sensitivity;\\n\"\n",
        "  \"Rights/consent per 1k with doc bootstrap CIs; neutral US state-appendix identification (overrideable regex);\\n\"\n",
        "  \"Modality markers (org discretion, user agency, user constraints). Figures are single-plot matplotlib PNG+SVG; captions as .txt.\\n\"\n",
        "  f\"Generated UTC: {datetime.datetime.now(datetime.timezone.utc).isoformat()}\\n\"\n",
        ")\n",
        "(OUT_SUPP/\"README.txt\").write_text(readme, encoding=\"utf-8\")\n",
        "\n",
        "# Manifest (sha256 + size for everything under outputs/)\n",
        "manifest = []\n",
        "for root, _, files in os.walk(\"outputs\"):\n",
        "    for fn in files:\n",
        "        p = Path(root)/fn\n",
        "        if p.is_file():\n",
        "            h = hashlib.sha256(p.read_bytes()).hexdigest()\n",
        "            manifest.append({\"path\": str(p.relative_to(\"outputs\")), \"sha256\": h, \"bytes\": p.stat().st_size})\n",
        "(OUT_META/\"manifest.json\").write_text(json.dumps({\"generated_at\": datetime.datetime.utcnow().isoformat()+\"Z\",\n",
        "                                                  \"manifest\": manifest}, indent=2),\n",
        "                                      encoding=\"utf-8\")\n",
        "\n",
        "# Bundle core supplement tables/figures/meta + README\n",
        "bundle = OUT_SUPP/\"supplement_bundle_tp_robustness.zip\"\n",
        "with zipfile.ZipFile(bundle, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    z.write(OUT_SUPP/\"README.txt\", arcname=\"README.txt\")\n",
        "    for sub in (\"figures\",\"tables\",\"meta\"):\n",
        "        base = Path(\"outputs\")/sub\n",
        "        for root, _, files in os.walk(base):\n",
        "            for fn in files:\n",
        "                p = Path(root)/fn\n",
        "                z.write(p, arcname=str(p.relative_to(\"outputs\")))\n",
        "\n",
        "sha = hashlib.sha256(bundle.read_bytes()).hexdigest()\n",
        "(OUT_SUPP/\"ZIP_SHA256.txt\").write_text(sha+\"\\n\", encoding=\"utf-8\")\n",
        "\n",
        "# ---------- FINAL SUMMARY ----------\n",
        "print(\"\\n==== FINAL SUMMARY ====\")\n",
        "print(meta[[\"company\",\"region\",\"n_paras\",\"n_words\"]].to_string(index=False))\n",
        "print(f\"\\nPrimary topic stability (K=6, seed=42): mean Jaccard = {mj_primary:.3f}\")\n",
        "print(f\"Classifier LOGO: accuracy = {acc_mean:.3f} ± {acc_sd:.3f} | macro-F1 = {f1_mean:.3f} ± {f1_sd:.3f}\")\n",
        "panel_out = (tab2.groupby(['company','region'])\n",
        "             .agg(rights_per_1000_RAW=('rights_per_1000_RAW','mean'))\n",
        "             .reset_index())\n",
        "print(\"\\nRights per 1k (document mean by company×region):\")\n",
        "print(panel_out.to_string(index=False))\n",
        "if not effects.empty:\n",
        "    print(\"\\nUS state-appendix deltas (per 1k):\")\n",
        "    print(effects[[\"company\",\"rights_raw\",\"rights_after\",\"flagged\"]].to_string(index=False))\n",
        "reg_print = (mod.groupby(\"region\")\n",
        "             .agg(org_discretion_per_1k=(\"org_discretion_per_1k\",\"mean\"),\n",
        "                  user_agency_per_1k=(\"user_agency_per_1k\",\"mean\"),\n",
        "                  user_constraints_per_1k=(\"user_constraints_per_1k\",\"mean\"))\n",
        "             .reset_index())\n",
        "print(\"\\nModality aggregates (mean per region):\")\n",
        "print(reg_print.to_string(index=False))\n",
        "print(\"\\nSaved figures → outputs/figures; tables → outputs/tables; meta → outputs/meta\")\n",
        "print(\"Supplement bundle:\", bundle)\n",
        "print(\"Supplement SHA256:\", sha)\n",
        "\n",
        "# Convenience: zip everything for download, try Colab auto-download\n",
        "ALL_ZIP = Path(f\"ai_policy_audit_ALL_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.zip\")\n",
        "with zipfile.ZipFile(ALL_ZIP, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
        "    for root, _, files in os.walk(\"outputs\"):\n",
        "        for fn in files:\n",
        "            p = Path(root)/fn\n",
        "            z.write(p, arcname=str(p))\n",
        "    # include source policy files\n",
        "    for stem in EXPECTED_STEMS:\n",
        "        for ext in (\".txt\",\".html\"):\n",
        "            p = Path(f\"{stem}{ext}\")\n",
        "            if p.exists(): z.write(p, arcname=str(p))\n",
        "try:\n",
        "    from google.colab import files\n",
        "    files.download(str(ALL_ZIP))\n",
        "except Exception:\n",
        "    pass"
      ]
    }
  ]
}